{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：  \n",
    "[HuggingFace-Transformers系列的下游应用](https://www.jianshu.com/p/cdb13530a8fd)  \n",
    "[官方文档的Quickstart](https://huggingface.co/transformers/quickstart.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个完整的transformer模型主要包含三部分：\n",
    "\n",
    "Config，控制模型的名称、最终输出的样式、隐藏层宽度和深度、激活函数的类别等。将Config类导出时文件格式为 json格式，就像下面这样：\n",
    "```\n",
    "{\n",
    "  \"architectures\": [\n",
    "    \"BertForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"bert\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"vocab_size\": 30522\n",
    "}\n",
    "\n",
    "```\n",
    "Tokenizer，将纯文本转换为编码。**Tokenizer并不涉及将词转化为词向量的过程，仅仅是将纯文本分词，添加`[MASK]`标记、`[SEP]`、`[CLS]`标记，并转换为字典索引**  \n",
    "Tokenizer类导出时将分为三个文件，也就是：  \n",
    "- vocab.txt 词典文件，每一行为一个词或词的一部分\n",
    "- special_tokens_map.json 特殊标记的定义方式\n",
    "```\n",
    "{\n",
    "\"unk_token\": \"[UNK]\", \n",
    "\"sep_token\": \"[SEP]\", \n",
    "\"pad_token\": \"[PAD]\", \n",
    "\"cls_token\": \"[CLS]\", \n",
    "\"mask_token\": \"[MASK]\"\n",
    "}\n",
    "```\n",
    "- tokenizer_config.json 配置文件，主要存储特殊的配置。\n",
    "\n",
    "Model，也就是各种各样的模型。除了初始的Bert、GPT等基本模型，针对下游任务，还定义了诸如BertForQuestionAnswering等下游任务模型。模型导出时将生成config.json和pytorch_model.bin参数文件。前者就是1中的配置文件,后者和torch.save()存储得到的文件是相同的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入模型\n",
    "### 通过官网自动导入(需要联网)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "# pytorch的方式\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tensorflow的方式\n",
    "# from transformers import BertTokenizer, TFBertModel\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = TFBertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从本地导入\n",
    "从 [HuggingFace官方模型库](https://huggingface.co/models) 找到需要下载的模型，如bert-base-uncased模型  \n",
    "点击 [Files and versions](https://huggingface.co/bert-base-uncased/tree/main)\n",
    "将其中的文件一一下载到同一目录中，如 D:/transformr_files/bert-base-uncased/\n",
    "```python\n",
    "from transformers import BertTokenizer,BertConfig,BertModel\n",
    "MODEL_PATH = r\"D:/transformr_files/bert-base-uncased/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(r\"D:/transformr_files/bert-base-uncased/bert-base-uncased-vocab.txt\") \n",
    "model_config = BertConfig.from_pretrained(MODEL_PATH)\n",
    "# 修改配置\n",
    "model_config.output_hidden_states = True\n",
    "model_config.output_attentions = True\n",
    "# 通过配置和路径导入模型\n",
    "model = BertModel.from_pretrained(MODEL_PATH, config = model_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_index = 8  # 第8个位置加入 '[MASK]'\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "# tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2040,\n",
       " 2001,\n",
       " 3958,\n",
       " 27227,\n",
       " 1029,\n",
       " 102,\n",
       " 3958,\n",
       " 103,\n",
       " 2001,\n",
       " 1037,\n",
       " 13997,\n",
       " 11510,\n",
       " 102]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词，str 转 id\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python list 转为 pytorch tensor\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各模型对应的输入格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert:       [CLS] + tokens + [SEP] + padding\n",
    "\n",
    "roberta:    [CLS] + prefix_space + tokens + [SEP] + padding\n",
    "\n",
    "distilbert: [CLS] + tokens + [SEP] + padding\n",
    "\n",
    "xlm:        [CLS] + tokens + [SEP] + padding\n",
    "\n",
    "xlnet:      padding + tokens + [SEP] + [CLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转到cuda上\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Transformers models 返回的outputs是一个元组\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    # 这里，返回的第一个元素是bert模型的最后一层的hidden_state\n",
    "    encoded_layers = outputs[0]\n",
    "# bert把input sequence 变成 shape= [batch_size, sequence_length, model_hidden_dimension] 的 FloatTensor\n",
    "assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs第二个元素的shape\n",
    "outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bert模型最后一层的hidden_state\n",
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)如果要更改模型本身的构建方式，则可以定义自定义配置类。  \n",
    "每种体系结构都有其自己的相关配置（对于DistilBERT，DistilBertConfig），它允许您指定任意hidden的维度，dropout率等。如果进行核心修改（如更改隐藏层的大小），则不能使用预训练模型，需要从头开始训练。 可以直接从自定义配置里实例化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用DistilBERT的预定义词汇表（即使用from_pretrained()方法加载tokenizer）  \n",
    "并从头开始初始化模型（即从配置中实例化模型，而不是使用from_pretrained()方法）\n",
    "```python\n",
    "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)\n",
    "model = DistilBertForSequenceClassification(config)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)仅改变模型头部（例如：标签数），仍可以对身体使用预训练模型。 例如，使用预训练的身体定义一个10分类的分类器。 我们可以创建一个配置，使用所有的默认值，只需更改标签的数量。  \n",
    "更容易地，您可以将携带任意参数的configuration直接传递给from_pretrained()方法，它将使用该参数更新默认配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "model =DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=10)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型的构建：\n",
    "# 一般情况下，一个基本模型对应一个Tokenizer, 所以并不存在对应于具体下游任务的Tokenizer。\n",
    "# 这里通过bert_model初始化BertForQuestionAnswering\n",
    "# from transformers import BertTokenizer,BertConfig, BertForQuestionAnswering\n",
    "# import torch\n",
    "\n",
    "# model = BertForQuestionAnswering.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "##  利用模型进行运算：\n",
    "# 设定模式\n",
    "# model.eval()\n",
    "# question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "# # 获取input_ids编码\n",
    "# input_ids = tokenizer.encode(question, text)\n",
    "# # 手动进行token_type_ids编码，可用encode_plus代替\n",
    "# token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n",
    "# # 得到评分\n",
    "# start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n",
    "# # 进行逆编码，得到原始的token \n",
    "# all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "\n",
    "## 将模型输出转化为任务输出：\n",
    "# 对输出的答案进行解码的过程\n",
    "# answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 快速浏览所有模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型|分词|预训练权重\n",
    "from transformers import *\n",
    "MODELS = [(BertModel, BertTokenizer, 'bert-base-uncased'),\n",
    "\n",
    "(OpenAIGPTModel, OpenAIGPTTokenizer, 'openai-gpt'),\n",
    "\n",
    "(GPT2Model, GPT2Tokenizer, 'gpt2'),\n",
    "\n",
    "(CTRLModel, CTRLTokenizer, 'ctrl'),\n",
    "\n",
    "(TransfoXLModel, TransfoXLTokenizer, 'transfo-xl-wt103'),\n",
    "\n",
    "(XLNetModel, XLNetTokenizer, 'xlnet-base-cased'),\n",
    "\n",
    "(XLMModel, XLMTokenizer, 'xlm-mlm-enfr-1024'),\n",
    "\n",
    "(DistilBertModel, DistilBertTokenizer, 'distilbert-base-cased'),\n",
    "\n",
    "(RobertaModel, RobertaTokenizer, 'roberta-base'),\n",
    "\n",
    "(XLMRobertaModel, XLMRobertaTokenizer, 'xlm-roberta-base'),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 用每个模型将一些文本编码成隐藏状态序列:\n",
    "for model_class, tokenizer_class, pretrained_weights in MODELS:\n",
    "    # 加载pretrained模型/分词器\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "    model = model_class.from_pretrained(pretrained_weights)\n",
    "    # 编码文本\n",
    "    input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)])  # 添加特殊标记\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids)[0]  # 模型输出是元组\n",
    "\n",
    "# 每个架构都提供了几个类，用于对下游任务进行调优，例如\n",
    "BERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,\n",
    "BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering]\n",
    "\n",
    "\n",
    "# 体系结构的所有类都可以从该体系结构的预训练权重开始\n",
    "# 注意：为微调添加的额外权重只在需要接受下游任务的训练时初始化\n",
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "for model_class in BERT_MODEL_CLASSES:\n",
    "    # 载入模型\n",
    "    # model = model_class.from_pretrained(pretrained_weights)\n",
    "    # 模型可以在每一层返回 隐藏状态和带有注意力机制的权值\n",
    "    model = model_class.from_pretrained(pretrained_weights, output_hidden_states=True, output_attentions=True)\n",
    "    input_ids = torch.tensor([tokenizer.encode(\"Let's see all hidden-states and attentions on this text\")])\n",
    "    all_hidden_states, all_attentions = model(input_ids)[-2:]\n",
    "\n",
    "    \n",
    "\n",
    "# 模型和分词的保存\n",
    "model.save_pretrained('./directory/to/save/') # 保存\n",
    "model = model_class.from_pretrained('./directory/to/save/') # 重载\n",
    "tokenizer.save_pretrained('./directory/to/save/') # 保存\n",
    "tokenizer = BertTokenizer.from_pretrained('./directory/to/save/') # 重载\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用albert_chinese_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer,BertModel,BertConfig\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878f3cdcb43f426fb17a29b35db6ce67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=109540.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c52641509b4a2481a0924dcf390ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=633.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89a33bf3fe34d15b9454f829590fac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=19258882.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at voidful/albert_chinese_small were not used when initializing BertModel: ['albert.embeddings.word_embeddings.weight', 'albert.embeddings.position_embeddings.weight', 'albert.embeddings.token_type_embeddings.weight', 'albert.embeddings.LayerNorm.weight', 'albert.embeddings.LayerNorm.bias', 'albert.encoder.embedding_hidden_mapping_in.weight', 'albert.encoder.embedding_hidden_mapping_in.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias', 'albert.pooler.weight', 'albert.pooler.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at voidful/albert_chinese_small and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pretrained = 'voidful/albert_chinese_small' #使用small版本Albert\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained)\n",
    "model = BertModel.from_pretrained(pretrained)\n",
    "config = BertConfig.from_pretrained(pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看albert模型的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 49, 384]), torch.Size([1, 384]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputtext = \"今天心情情很好啊，买了很多东西，我特别喜欢，终于有了自己喜欢的电子产品，这次总算可以好好学习了\"\n",
    "tokenized_text = tokenizer.encode(inputtext)\n",
    "input_ids = torch.tensor(tokenized_text).view(-1,len(tokenized_text))\n",
    "outputs = model(input_ids)\n",
    "outputs[0].shape,outputs[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在albert后面接自定义的线性层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertClassfier(torch.nn.Module):\n",
    "    def __init__(self, bert_model, bert_config, num_class):\n",
    "        super(AlbertClassfier,self).__init__()\n",
    "        self.bert_model=bert_model\n",
    "        self.dropout=torch.nn.Dropout(0.4)\n",
    "        self.fc1=torch.nn.Linear(bert_config.hidden_size,bert_config.hidden_size)\n",
    "        self.fc2=torch.nn.Linear(bert_config.hidden_size,num_class)\n",
    "    def forward(self,token_ids):\n",
    "        bert_out=self.bert_model(token_ids)[1] #句向量 [batch_size,hidden_size]\n",
    "        bert_out=self.dropout(bert_out)\n",
    "        bert_out=self.fc1(bert_out) \n",
    "        bert_out=self.dropout(bert_out)\n",
    "        bert_out=self.fc2(bert_out) #[batch_size,num_class]\n",
    "        return bert_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "albertBertClassifier = AlbertClassfier(model,config, 2)\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else 'cpu'\n",
    "albertBertClassifier = albertBertClassifier.to(device)  # 转为GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1146,  0.1103, -0.0149,  ..., -0.0809, -0.0018, -0.0271],\n",
       "         [-0.0225,  0.1612,  0.0556,  ...,  0.5366,  0.1196,  0.1576],\n",
       "         [ 0.0532, -0.0020,  0.0370,  ..., -0.4887,  0.1641,  0.2736],\n",
       "         ...,\n",
       "         [-0.1586,  0.0837,  0.1302,  ...,  0.3970,  0.1715, -0.0848],\n",
       "         [-0.1065,  0.1044, -0.0383,  ..., -0.1068, -0.0015, -0.0517],\n",
       "         [ 0.0059,  0.0758,  0.1228,  ...,  0.1037,  0.0075,  0.0976]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-2.8347e-03, -1.8850e-01, -2.1461e-01, -1.1530e-01,  1.3189e-01,\n",
       "          2.3539e-01,  2.7159e-01, -6.0856e-02, -8.3708e-02, -1.9298e-01,\n",
       "          2.6565e-01, -4.3355e-05, -1.1200e-01,  1.4636e-01, -1.4502e-01,\n",
       "          4.9390e-01,  2.0317e-01, -5.3051e-01,  7.5674e-02, -3.7784e-02,\n",
       "         -2.8706e-01,  7.9115e-02,  4.9498e-01,  3.6626e-01,  1.0755e-01,\n",
       "          4.5707e-02, -1.5804e-01,  1.3338e-02,  1.5470e-01,  2.6411e-01,\n",
       "          3.0425e-01,  3.5527e-02,  1.1397e-01,  2.5932e-01, -2.5477e-01,\n",
       "          6.6820e-02, -3.4011e-01,  8.4366e-03,  2.8969e-01, -1.9251e-01,\n",
       "         -8.1795e-02,  1.7997e-01,  1.9420e-01, -1.8720e-01, -1.0835e-01,\n",
       "          4.2660e-01,  2.5290e-01,  4.4049e-02, -1.5877e-01, -9.2707e-02,\n",
       "         -3.3307e-01,  3.7266e-01,  3.0196e-01,  2.2397e-01, -6.3288e-02,\n",
       "          4.3699e-02, -9.3048e-02,  2.8365e-01, -7.5851e-02, -7.6650e-02,\n",
       "         -1.2443e-01, -2.3357e-01,  7.3141e-03, -6.6388e-02,  4.7961e-02,\n",
       "         -1.3959e-01,  1.0733e-01, -1.6085e-01, -1.3563e-01,  3.1196e-02,\n",
       "         -1.1300e-01,  1.7004e-01,  1.6964e-01, -3.3313e-01, -3.1102e-01,\n",
       "          7.0277e-02, -6.3457e-01, -9.6144e-02,  3.4468e-01,  4.6423e-01,\n",
       "         -8.3255e-02,  2.4088e-01,  3.9045e-02,  2.1656e-01, -1.2806e-02,\n",
       "         -7.3343e-02, -4.6851e-02, -1.4373e-01,  1.4100e-01,  2.6473e-01,\n",
       "         -2.1426e-01, -4.6240e-01,  4.3216e-02,  3.9133e-02, -9.2348e-02,\n",
       "          2.2254e-02, -1.1372e-02, -1.1127e-01, -1.8471e-01, -1.8038e-01,\n",
       "          9.9089e-02, -2.4278e-01, -1.5152e-01,  2.6573e-01,  4.7385e-04,\n",
       "         -1.6355e-01,  8.2377e-04,  3.2038e-01,  5.0321e-02, -1.0096e-01,\n",
       "         -2.1142e-01,  4.7146e-01,  3.5390e-01, -3.4947e-02, -1.2649e-03,\n",
       "          1.7674e-01,  1.5086e-01, -2.9137e-01,  4.5807e-01, -3.3300e-01,\n",
       "         -3.9866e-03, -1.0464e-01,  1.4464e-01,  1.9178e-01, -2.0823e-01,\n",
       "          3.0469e-01,  1.5175e-01,  3.0945e-01,  2.1146e-01,  1.4162e-01,\n",
       "         -4.4261e-02,  1.3641e-01, -1.3945e-01,  1.5955e-01,  2.3872e-01,\n",
       "          1.2040e-01, -4.9609e-03, -3.4112e-01, -2.6020e-01,  3.0965e-01,\n",
       "          3.4374e-01,  1.3289e-01, -6.3243e-02,  2.0233e-01,  1.0470e-01,\n",
       "          2.4754e-01,  1.4736e-01, -4.4511e-01,  4.2347e-02,  3.5871e-01,\n",
       "          1.2213e-01,  1.5714e-01, -1.1579e-01, -3.0872e-01, -2.8117e-01,\n",
       "         -1.1094e-01,  5.6936e-02, -3.6070e-01, -1.2276e-01,  3.9022e-01,\n",
       "          8.4130e-02, -7.2016e-02, -1.9839e-01, -2.2041e-01, -1.1240e-02,\n",
       "         -1.2389e-01,  6.9023e-03,  9.8317e-02, -7.9866e-02, -4.6790e-01,\n",
       "         -1.3635e-01, -5.3511e-01, -9.6947e-02,  2.2063e-01, -3.5547e-01,\n",
       "          3.0791e-01, -2.9423e-01,  1.0540e-01,  4.1336e-01,  5.9913e-02,\n",
       "         -1.8542e-02, -2.4088e-01, -1.6369e-02,  9.3561e-02,  3.2514e-01,\n",
       "          2.9757e-01, -4.3569e-01,  1.0774e-01,  1.4302e-01,  3.1605e-01,\n",
       "          1.1054e-01, -8.9299e-02, -1.1941e-01,  1.7482e-01, -2.1888e-01,\n",
       "          1.7849e-01, -2.2932e-01,  2.0647e-01, -2.5988e-01, -2.5896e-01,\n",
       "          3.1579e-01, -4.5348e-01, -7.6572e-02,  8.1546e-02,  2.5454e-01,\n",
       "          6.5838e-03, -5.5890e-02, -1.0195e-01,  1.8580e-01,  1.7586e-01,\n",
       "          1.2009e-01, -4.2580e-01,  3.1833e-01, -3.5225e-02, -3.6717e-02,\n",
       "         -3.6468e-02,  1.8630e-01,  2.5572e-01,  9.9702e-02, -3.8584e-01,\n",
       "         -1.5646e-01,  1.3589e-01,  3.1458e-01, -2.6697e-01,  1.6693e-01,\n",
       "         -3.3316e-01, -4.3700e-01, -1.1081e-01,  2.4861e-01,  2.2493e-01,\n",
       "          1.9945e-01, -3.0226e-01,  1.7410e-01, -1.5682e-01, -4.5891e-01,\n",
       "         -3.8876e-01, -1.3655e-01,  2.3969e-01,  1.7983e-01,  1.6895e-01,\n",
       "          2.5715e-01,  3.0751e-02,  1.1278e-01,  1.6356e-01,  1.6140e-01,\n",
       "         -1.3235e-01,  1.5606e-01, -3.6884e-01, -4.3860e-02, -3.2912e-01,\n",
       "         -2.1013e-01, -2.2653e-01,  4.2579e-01, -2.5797e-01,  2.6423e-01,\n",
       "          4.3591e-01, -3.3986e-01, -1.0784e-01,  1.4810e-01,  1.8771e-01,\n",
       "          5.8792e-02, -9.3627e-02,  2.0970e-01,  2.0622e-01, -1.0515e-01,\n",
       "          2.8239e-01, -4.3976e-02,  2.9693e-01,  1.5088e-01,  4.6862e-02,\n",
       "          1.6996e-01,  1.3566e-01, -1.3045e-01,  8.1460e-02,  1.2664e-02,\n",
       "         -3.3464e-02, -2.8986e-01, -1.4230e-01,  2.3894e-01, -7.3702e-02,\n",
       "          2.3693e-02, -1.6792e-01, -2.9238e-02,  1.1320e-02,  4.2676e-01,\n",
       "         -3.7155e-01,  2.7734e-01,  5.9552e-02,  1.4843e-01, -2.3717e-01,\n",
       "         -2.4174e-01,  1.2127e-01,  2.3405e-01, -4.7163e-01,  4.4663e-02,\n",
       "          1.1726e-01,  9.0709e-02,  2.1785e-01,  2.8467e-01,  4.3690e-04,\n",
       "         -1.1491e-01,  5.6653e-01, -1.5376e-01, -1.3448e-01,  2.6769e-01,\n",
       "         -3.0039e-01, -3.0043e-01,  2.7399e-01, -1.9757e-02,  3.1912e-01,\n",
       "          1.4622e-01,  5.6607e-02,  6.1616e-02, -6.1817e-01,  8.0240e-02,\n",
       "         -4.8122e-01, -1.5592e-02,  5.3547e-02, -8.4474e-02, -2.3283e-01,\n",
       "          1.6090e-01,  3.1869e-01, -2.3078e-01, -1.9177e-02,  2.1890e-01,\n",
       "          4.8886e-02, -1.2004e-01,  4.8328e-01, -1.8756e-03,  2.2632e-01,\n",
       "         -5.1116e-02,  2.3901e-01, -2.3146e-01,  2.6983e-01, -2.9126e-01,\n",
       "         -1.2003e-01,  6.2455e-02,  7.8869e-02,  4.1980e-02, -7.5116e-02,\n",
       "         -3.9176e-01,  2.5678e-01, -1.1382e-02, -7.3775e-02, -5.6824e-02,\n",
       "          1.0898e-01, -2.5817e-02,  7.3328e-02,  8.8274e-02,  3.6668e-01,\n",
       "          2.4559e-01, -1.2493e-02, -4.1410e-01, -8.9574e-02, -1.2473e-01,\n",
       "          7.0269e-02,  6.0476e-02,  1.4108e-02,  4.8732e-01, -7.1245e-02,\n",
       "          2.6076e-02, -1.5084e-01,  2.8914e-01,  2.2343e-01,  1.7564e-01,\n",
       "          1.6365e-01,  5.4786e-02,  1.8040e-01, -4.7131e-02, -1.8931e-02,\n",
       "         -1.5203e-01, -2.5458e-01, -2.9443e-01,  2.3336e-01, -2.5514e-01,\n",
       "         -1.6871e-01,  1.6014e-01,  2.0769e-01, -1.6111e-01,  1.8165e-01,\n",
       "          3.4196e-01,  1.5221e-01, -1.6557e-01,  3.1800e-01, -7.6828e-02,\n",
       "          6.5035e-02,  3.0127e-01, -8.3677e-02,  2.0537e-01,  5.4490e-01,\n",
       "          2.5054e-01, -4.2671e-01, -5.8351e-02, -2.2732e-01,  1.4475e-02,\n",
       "          2.7506e-01, -1.2389e-01,  2.1066e-01,  3.8047e-01,  3.0992e-01,\n",
       "          4.9357e-01, -1.5483e-02, -1.1453e-01,  1.1932e-01,  2.3738e-01,\n",
       "          2.3809e-02, -2.0668e-01, -1.7490e-01,  3.0054e-01,  9.5040e-02,\n",
       "         -1.6448e-01,  8.2032e-03, -1.6251e-01,  4.5058e-02, -1.6121e-01,\n",
       "         -4.2871e-01,  4.4034e-02,  1.6746e-01, -5.0884e-01,  1.2617e-01,\n",
       "         -3.1699e-01,  3.6625e-02, -2.4639e-01,  2.6873e-01, -2.5727e-01,\n",
       "         -1.4897e-01,  4.3529e-01, -5.5864e-02,  2.7185e-02, -1.9407e-01,\n",
       "         -1.6114e-01,  2.9191e-02, -1.6526e-03, -7.2601e-02, -4.9691e-03,\n",
       "          3.5502e-01, -1.4824e-01,  7.0600e-02,  2.8513e-02,  2.1469e-01,\n",
       "         -7.3574e-02,  1.7018e-01, -2.6029e-03, -1.4642e-01, -4.1492e-01,\n",
       "          1.6521e-01, -2.2469e-01, -4.2976e-01, -4.2113e-01,  4.1487e-01,\n",
       "         -1.7290e-01, -2.8811e-01, -2.2153e-01, -2.5111e-01,  5.3529e-02,\n",
       "          2.4050e-01,  4.7290e-01, -4.0037e-01, -5.9299e-02,  4.9183e-01,\n",
       "         -5.9624e-02, -2.3099e-01,  2.7614e-01,  2.0807e-01, -3.2530e-01,\n",
       "          3.7258e-01,  2.6673e-01, -6.3106e-02,  5.1716e-02,  5.4289e-01,\n",
       "          1.1893e-01,  2.1599e-01, -2.1227e-01,  4.7760e-01, -2.6859e-01,\n",
       "          3.2262e-01, -1.7062e-01, -1.9771e-01, -2.4215e-01, -3.1962e-02,\n",
       "          3.6125e-01,  1.9359e-01, -4.5894e-01, -1.3200e-01,  4.3003e-02,\n",
       "          3.3126e-01, -4.2145e-01, -6.0110e-02,  4.1000e-02, -3.6422e-01,\n",
       "          1.2588e-01,  1.3854e-01,  2.4897e-01, -4.2243e-01, -6.7698e-02,\n",
       "          4.1120e-01, -3.4483e-01,  1.3896e-01,  3.0409e-01,  8.2978e-02,\n",
       "          3.6610e-01, -6.5310e-02, -8.2796e-03,  5.0149e-02, -2.6011e-01,\n",
       "         -2.2435e-02,  1.2717e-01,  5.8494e-01,  1.4487e-01, -4.0354e-01,\n",
       "          1.1067e-01,  2.6653e-01, -1.8659e-01,  3.3972e-01, -1.0567e-01,\n",
       "         -3.7917e-02,  2.6797e-01, -5.2882e-02,  1.6860e-01, -9.7774e-02,\n",
       "         -2.3714e-01, -3.5562e-01,  4.1702e-01, -1.9961e-01, -1.1364e-01,\n",
       "         -2.0353e-01, -1.0914e-01, -1.9304e-01,  9.5546e-03, -3.9396e-01,\n",
       "          3.6519e-01,  1.3993e-01, -2.1809e-01, -9.8051e-02, -1.0919e-01,\n",
       "         -2.0547e-01, -2.0266e-01, -2.9378e-01,  4.2927e-01, -1.8953e-01,\n",
       "         -4.9567e-01,  2.4299e-01, -1.3043e-02,  3.6800e-01,  4.0743e-03,\n",
       "          1.0481e-01, -6.7964e-02,  1.2875e-01,  8.9863e-02, -8.7973e-02,\n",
       "          2.8249e-01,  9.1169e-02, -5.9373e-01, -1.3107e-01, -2.6811e-01,\n",
       "          9.8535e-02,  2.3464e-01, -3.9614e-01,  1.5656e-02,  3.6724e-02,\n",
       "          2.0153e-01,  1.1375e-02, -1.2173e-01, -7.0875e-02,  4.3149e-01,\n",
       "          2.6859e-01,  3.0436e-01,  1.3308e-01,  2.3106e-01, -4.5165e-03,\n",
       "         -3.6247e-01,  1.8061e-02,  9.4915e-02, -1.9820e-01,  4.6976e-01,\n",
       "         -1.2277e-01, -3.9047e-01, -7.1323e-02,  4.2497e-01,  9.7055e-02,\n",
       "         -2.6554e-02, -3.3067e-02,  2.3281e-01,  1.9053e-01, -1.2797e-01,\n",
       "          2.1960e-01, -8.9729e-03, -1.4726e-01, -1.5024e-01,  9.6500e-02,\n",
       "         -2.3059e-01,  3.7448e-02, -1.4284e-01, -2.9569e-03, -2.2259e-01,\n",
       "          9.0303e-03, -2.2012e-01,  3.0411e-01, -3.5056e-01,  1.1357e-01,\n",
       "          3.1748e-02,  3.2109e-01, -3.7659e-01, -1.7942e-01, -2.8157e-02,\n",
       "          2.0363e-01,  2.7066e-01,  3.8169e-01,  2.1824e-02,  5.1432e-02,\n",
       "         -2.0284e-01, -2.8634e-01,  5.2265e-02, -2.1432e-01,  1.1748e-01,\n",
       "          8.6978e-02,  2.7207e-01, -3.1238e-01, -2.0362e-01,  2.3377e-01,\n",
       "         -5.4622e-02, -1.4171e-01,  4.9318e-01,  2.5240e-01,  1.8076e-01,\n",
       "          2.9939e-02,  2.3065e-01,  2.8543e-02, -1.6701e-01, -1.1991e-01,\n",
       "         -3.0575e-01,  1.0910e-01, -1.1004e-01, -4.9718e-02, -6.8488e-02,\n",
       "         -1.1613e-01, -2.6441e-01, -1.9368e-01,  1.5138e-01,  1.4259e-01,\n",
       "          3.0952e-02, -5.1871e-02, -1.8964e-02, -3.0977e-01,  3.2418e-01,\n",
       "          1.8257e-02,  1.1345e-01, -3.2933e-02,  5.4759e-02, -1.6449e-01,\n",
       "          2.4663e-01,  2.1938e-01,  5.0557e-02, -2.0592e-01, -4.8105e-02,\n",
       "         -3.3525e-01, -3.7126e-01,  2.9869e-02,  1.6168e-01,  1.3227e-01,\n",
       "         -9.2244e-02, -2.8055e-01,  5.7758e-03, -1.2485e-01,  1.9057e-01,\n",
       "          1.8671e-02, -1.8965e-01, -8.9001e-02, -7.3102e-02, -2.7037e-02,\n",
       "          9.7274e-02, -2.2742e-01, -2.0173e-01, -1.3191e-01, -8.9041e-02,\n",
       "         -8.0196e-02,  3.6282e-01, -8.2313e-02,  3.2301e-01, -1.6621e-01,\n",
       "          2.9981e-02, -2.1863e-01,  1.5416e-01, -7.0528e-02,  7.8769e-02,\n",
       "          3.0648e-01, -4.6061e-01, -1.2646e-01, -2.7547e-02, -1.9933e-01,\n",
       "         -1.6299e-01, -1.1201e-01, -1.8919e-02,  2.2978e-01, -3.7508e-01,\n",
       "          2.0549e-01, -1.2558e-01,  2.0741e-01, -7.4029e-02, -2.5238e-01,\n",
       "         -1.6511e-01, -1.0580e-03,  2.9722e-01, -3.3988e-01, -2.3128e-01,\n",
       "         -3.1352e-01, -1.2131e-01, -1.1570e-01, -2.9116e-01,  4.4779e-01,\n",
       "         -1.1711e-01, -5.9108e-02,  3.8968e-02,  4.3492e-01,  2.1890e-01,\n",
       "          1.7709e-01,  2.5733e-01,  1.5447e-02,  1.2344e-02,  1.1580e-01,\n",
       "         -4.9933e-01,  2.8145e-01, -2.6542e-01, -1.7272e-01,  2.0931e-04,\n",
       "          1.4980e-01, -5.8473e-02,  3.9033e-02, -1.5795e-01, -1.1659e-01,\n",
       "          2.3245e-01, -3.9180e-01, -1.6454e-02,  3.0201e-01,  1.5689e-01,\n",
       "         -2.9235e-01,  3.0020e-02,  1.2322e-01,  3.8744e-01,  5.0056e-02,\n",
       "         -2.4898e-01,  1.4628e-01, -3.6670e-01, -1.9514e-02, -2.1036e-01,\n",
       "         -3.3647e-01,  1.9012e-01, -1.1920e-01,  3.9577e-02, -6.7328e-02,\n",
       "         -3.0076e-01,  2.0926e-01, -6.3638e-02, -3.9904e-02,  4.5228e-01,\n",
       "          7.4620e-02, -1.1017e-01,  1.5910e-01,  3.6012e-02,  1.9745e-02,\n",
       "         -9.8928e-02,  2.8636e-01,  2.0934e-01, -3.2661e-01,  7.1475e-02,\n",
       "         -1.7098e-01, -2.6441e-02, -1.4120e-01]], grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
